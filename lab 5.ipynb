{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXrDbWvAnmlcx3ABee9qW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a51420/reinforcement/blob/main/lab%205.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": [],\n",
        "      \"authorship_tag\": \"ABX9TyOymC+jmASj24GAG1/9MUXC\",\n",
        "      \"include_colab_link\": True\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"view-in-github\",\n",
        "        \"colab_type\": \"text\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"<a href=\\\"https://colab.research.google.com/github/Goutham345/Reinforcement_Learning/blob/main/lab-5.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"execution_count\": 3,\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\"\n",
        "        },\n",
        "        \"id\": \"i2KehKKYBT4R\",\n",
        "        \"outputId\": \"b3003792-69de-46d0-f108-a1f8add4ea70\"\n",
        "      },\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"Using backend: gymnasium\\n\",\n",
        "            \"Training FrozenLake-v1 | episodes=20000, alpha=0.8, gamma=0.95\\n\",\n",
        "            \"[  2000/20000] avg_return(last 200) = 0.035, epsilon=0.368\\n\",\n",
        "            \"[  4000/20000] avg_return(last 200) = 0.145, epsilon=0.135\\n\",\n",
        "            \"[  6000/20000] avg_return(last 200) = 0.205, epsilon=0.050\\n\",\n",
        "            \"[  8000/20000] avg_return(last 200) = 0.400, epsilon=0.018\\n\",\n",
        "            \"[ 10000/20000] avg_return(last 200) = 0.440, epsilon=0.010\\n\",\n",
        "            \"[ 12000/20000] avg_return(last 200) = 0.425, epsilon=0.010\\n\",\n",
        "            \"[ 14000/20000] avg_return(last 200) = 0.495, epsilon=0.010\\n\",\n",
        "            \"[ 16000/20000] avg_return(last 200) = 0.500, epsilon=0.010\\n\",\n",
        "            \"[ 18000/20000] avg_return(last 200) = 0.540, epsilon=0.010\\n\",\n",
        "            \"[ 20000/20000] avg_return(last 200) = 0.375, epsilon=0.010\\n\",\n",
        "            \"\\n\",\n",
        "            \"Evaluation (greedy policy):\\n\",\n",
        "            \"Average reward over 100 episodes: 0.240\\n\",\n",
        "            \"Average steps to termination: 16.1\\n\",\n",
        "            \"\\n\",\n",
        "            \"Tips for FrozenLake:\\n\",\n",
        "            \"- Slippery=True → Use >=20k episodes and slower epsilon decay (0.9997).\\n\",\n",
        "            \"- Slippery=False → 3k–10k episodes often suffice.\\n\",\n",
        "            \"\\n\",\n",
        "            \"Saved: q_table.npy, returns.npy, epsilons.npy\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ],\n",
        "      \"source\": [\n",
        "        \"import argparse\\n\",\n",
        "        \"import sys\\n\",\n",
        "        \"import math\\n\",\n",
        "        \"import numpy as np\\n\",\n",
        "        \"import random\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Try Gymnasium first; fallback to Gym\\n\",\n",
        "        \"try:\\n\",\n",
        "        \"    import gymnasium as gym\\n\",\n",
        "        \"    GYMN = \\\"gymnasium\\\"\\n\",\n",
        "        \"except Exception:\\n\",\n",
        "        \"    import gym\\n\",\n",
        "        \"    GYMN = \\\"gym\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def make_env(env_id: str,\\n\",\n",
        "        \"             is_slippery: bool | None = None,\\n\",\n",
        "        \"             render_mode: str | None = None,\\n\",\n",
        "        \"             seed: int | None = 42):\\n\",\n",
        "        \"    \\\"\\\"\\\"Create an environment with sensible defaults for FrozenLake and Taxi.\\\"\\\"\\\"\\n\",\n",
        "        \"    kwargs = {}\\n\",\n",
        "        \"    if render_mode is not None:\\n\",\n",
        "        \"        kwargs[\\\"render_mode\\\"] = render_mode\\n\",\n",
        "        \"    if env_id.startswith(\\\"FrozenLake\\\") and is_slippery is not None:\\n\",\n",
        "        \"        kwargs[\\\"is_slippery\\\"] = bool(is_slippery)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    env = gym.make(env_id, **kwargs)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    try:\\n\",\n",
        "        \"        env.reset(seed=seed)\\n\",\n",
        "        \"    except TypeError:\\n\",\n",
        "        \"        pass\\n\",\n",
        "        \"    if hasattr(env, \\\"action_space\\\") and hasattr(env.action_space.seed):\\n\",\n",
        "        \"        env.action_space.seed(seed)\\n\",\n",
        "        \"    if hasattr(env, \\\"observation_space\\\") and hasattr(env.observation_space.n, \\\"seed\\\"):\\n\",\n",
        "        \"        env.observation_space.seed(seed)\\n\",\n",
        "        \"    return env\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def toint(x) -> int:\\n\",\n",
        "        \"    return int(x) if not isinstance(x, (tuple, list, np.ndarray)) else int(x[0])\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def greedyaction(q_row: np.ndarray) -> int:\\n\",\n",
        "        \"    max_val = np.max(q_row)\\n\",\n",
        "        \"    best_acts = np.flatnonzero(q_row == max_val)\\n\",\n",
        "        \"    return int(np.random.choice(best_acts))\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def epsilon_greedy_action(q_table: np.ndarray, state: int, epsilon: float, n_actions: int) -> int:\\n\",\n",
        "        \"    if random.random() < epsilon:\\n\",\n",
        "        \"        return random.randrange(n_actions)\\n\",\n",
        "        \"    return greedyaction(q_table[state])\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def stepunpack(step_out):\\n\",\n",
        "        \"    \\\"\\\"\\\"Unpack step results for Gymnasium (5-tuple) and Gym (4-tuple).\\\"\\\"\\\"\\n\",\n",
        "        \"    if isinstance(step_out, tuple):\\n\",\n",
        "        \"        if len(step_out) == 5:\\n\",\n",
        "        \"            next_state, reward, terminated, truncated, info = step_out\\n\",\n",
        "        \"            done = bool(terminated) or bool(truncated)\\n\",\n",
        "        \"            return next_state, reward, done\\n\",\n",
        "        \"        elif len(step_out) == 4:\\n\",\n",
        "        \"            next_state, reward, done, info = step_out\\n\",\n",
        "        \"            return next_state, reward, bool(done)\\n\",\n",
        "        \"    raise RuntimeError(\\\"Unexpected env.step(...) return format.\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def resetobs(reset_out):\\n\",\n",
        "        \"    return reset_out[0] if isinstance(reset_out, tuple) else reset_out\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def train_q_learning(env_id: str = \\\"FrozenLake-v1\\\",\\n\",\n",
        "        \"                     episodes: int = 20000,\\n\",\n",
        "        \"                     max_steps: int = 200,\\n\",\n",
        "        \"                     alpha: float = 0.8,\\n\",\n",
        "        \"                     gamma: float = 0.95,\\n\",\n",
        "        \"                     epsilon: float = 1.0,\\n\",\n",
        "        \"                     epsilon_min: float = 0.01,\\n\",\n",
        "        \"                     epsilon_decay: float = 0.9995,\\n\",\n",
        "        \"                     is_slippery: bool | None = None,\\n\",\n",
        "        \"                     seed: int = 42,\\n\",\n",
        "        \"                     verbose: bool = True):\\n\",\n",
        "        \"    env = make_env(env_id, is_slippery=is_slippery, seed=seed)\\n\",\n",
        "        \"    assert hasattr(env.observation_space, 'n') and hasattr(env.action_space, 'n'), \\\\\\n\",\n",
        "        \"        \\\"This Q-learning implementation expects discrete state and action spaces.\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"    n_states = int(env.observation_space.n)\\n\",\n",
        "        \"    n_actions = int(env.action_space.n)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    q_table = np.zeros((n_states, n_actions), dtype=np.float32)\\n\",\n",
        "        \"    returns = np.zeros(episodes, dtype=np.float32)\\n\",\n",
        "        \"    epsilons = np.zeros(episodes, dtype=np.float32)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    for ep in range(episodes):\\n\",\n",
        "        \"        reset_out = env.reset(seed=seed + ep)\\n\",\n",
        "        \"        state = toint(resetobs(reset_out))\\n\",\n",
        "        \"\\n\",\n",
        "        \"        total_reward = 0.0\\n\",\n",
        "        \"        for t in range(max_steps):\\n\",\n",
        "        \"            action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\\n\",\n",
        "        \"            step_out = env.step(action)\\n\",\n",
        "        \"            next_state, reward, done = stepunpack(step_out)\\n\",\n",
        "        \"\\n\",\n",
        "        \"            s = toint(state)\\n\",\n",
        "        \"            s_next = toint(next_state)\\n\",\n",
        "        \"\\n\",\n",
        "        \"            # Q-learning update\\n\",\n",
        "        \"            best_next = float(np.max(q_table[s_next]))\\n\",\n",
        "        \"            td_target = float(reward) + (0.0 if done else gamma * best_next)\\n\",\n",
        "        \"            td_error = td_target - q_table[s, action]\\n\",\n",
        "        \"            q_table[s, action] += alpha * td_error\\n\",\n",
        "        \"\\n\",\n",
        "        \"            state = s_next\\n\",\n",
        "        \"            total_reward += float(reward)\\n\",\n",
        "        \"            if done:\\n\",\n",
        "        \"                break\\n\",\n",
        "        \"\\n\",\n",
        "        \"        returns[ep] = total_reward\\n\",\n",
        "        \"        epsilons[ep] = epsilon\\n\",\n",
        "        \"        epsilon = max(epsilon_min, epsilon * epsilon_decay)\\n\",\n",
        "        \"\\n\",\n",
        "        \"        if verbose and (ep + 1) % max(1, episodes // 10) == 0:\\n\",\n",
        "        \"            window = 200 if episodes >= 200 else max(1, episodes // 5)\\n\",\n",
        "        \"            avg_recent = float(np.mean(returns[max(0, ep - window + 1):ep + 1]))\\n\",\n",
        "        \"            print(f\\\"[{ep+1:6d}/{episodes}] avg_return(last {window}) = {avg_recent:.3f}, epsilon={epsilon:.3f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    env.close()\\n\",\n",
        "        \"    return q_table, returns, epsilons\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def evaluate_policy(env_id: str,\\n\",\n",
        "        \"                    q_table: np.ndarray,\\n\",\n",
        "        \"                    episodes: int = 100,\\n\",\n",
        "        \"                    max_steps: int = 200,\\n\",\n",
        "        \"                    is_slippery: bool | None = None,\\n\",\n",
        "        \"                    seed: int = 9999):\\n\",\n",
        "        \"    env = make_env(env_id, is_slippery=is_slippery, render_mode=None, seed=seed)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    total_rewards = []\\n\",\n",
        "        \"    steps_taken = []\\n\",\n",
        "        \"\\n\",\n",
        "        \"    for ep in range(episodes):\\n\",\n",
        "        \"        reset_out = env.reset(seed=seed + ep)\\n\",\n",
        "        \"        state = toint(resetobs(reset_out))\\n\",\n",
        "        \"        ep_reward = 0.0\\n\",\n",
        "        \"        for t in range(max_steps):\\n\",\n",
        "        \"            action = greedyaction(q_table[state])\\n\",\n",
        "        \"            step_out = env.step(action)\\n\",\n",
        "        \"            next_state, reward, done = stepunpack(step_out)\\n\",\n",
        "        \"\\n\",\n",
        "        \"            ep_reward += float(reward)\\n\",\n",
        "        \"            state = toint(next_state)\\n\",\n",
        "        \"            if done:\\n\",\n",
        "        \"                steps_taken.append(t + 1)\\n\",\n",
        "        \"                break\\n\",\n",
        "        \"        else:\\n\",\n",
        "        \"            steps_taken.append(max_steps)\\n\",\n",
        "        \"        total_rewards.append(ep_reward)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    env.close()\\n\",\n",
        "        \"    return float(np.mean(total_rewards)), float(np.mean(steps_taken))\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def main():\\n\",\n",
        "        \"    parser = argparse.ArgumentParser(description=\\\"Tabular Q-Learning for discrete Gym environments\\\")\\n\",\n",
        "        \"    parser.add_argument('--env', type=str, default='FrozenLake-v1')\\n\",\n",
        "        \"    parser.add_argument('--episodes', type=int, default=20000)\\n\",\n",
        "        \"    parser.add_argument('--max_steps', type=int, default=200)\\n\",\n",
        "        \"    parser.add_argument('--alpha', type=float, default=0.8)\\n\",\n",
        "        \"    parser.add_argument('--gamma', type=float, default=0.95)\\n\",\n",
        "        \"    parser.add_argument('--epsilon', type=float, default=1.0)\\n\",\n",
        "        \"    parser.add_argument('--epsilon_min', type=float, default=0.01)\\n\",\n",
        "        \"    parser.add_argument('--epsilon_decay', type=float, default=0.9995)\\n\",\n",
        "        \"    parser.add_argument('--is_slippery', type=int, default=None, choices=[0, 1])\\n\",\n",
        "        \"    parser.add_argument('--seed', type=int, default=42)\\n\",\n",
        "        \"    parser.add_argument('--no_verbose', action='store_true')\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # FIX for Jupyter/Colab extra arguments\\n\",\n",
        "        \"    args, _ = parser.parse_known_args()\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(f\\\"Using backend: {GYMN}\\\")\\n\",\n",
        "        \"    print(f\\\"Training {args.env} | episodes={args.episodes}, alpha={args.alpha}, gamma={args.gamma}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    q_table, returns, eps = train_q_learning(env_id=args.env,\\n\",\n",
        "        \"                                             episodes=args.episodes,\\n\",\n",
        "        \"                                             max_steps=args.max_steps,\\n\",\n",
        "        \"                                             alpha=args.alpha,\\n\",\n",
        "        \"                                             gamma=args.gamma,\\n\",\n",
        "        \"                                             epsilon=args.epsilon,\\n\",\n",
        "        \"                                             epsilon_min=args.epsilon_min,\\n\",\n",
        "        \"                                             epsilon_decay=args.epsilon_decay,\\n\",\n",
        "        \"                                             is_slippery=(None if args.is_slippery is None else bool(args.is_slippery)),\\n\",\n",
        "        \"                                             seed=args.seed,\\n\",\n",
        "        \"                                             verbose=not args.no_verbose)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    avg_reward, avg_steps = evaluate_policy(env_id=args.env,\\n\",\n",
        "        \"                                            q_table=q_table,\\n\",\n",
        "        \"                                            max_steps=args.max_steps,\\n\",\n",
        "        \"                                            is_slippery=(None if args.is_slippery is None else bool(args.is_slippery)))\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"\\\\nEvaluation (greedy policy):\\\")\\n\",\n",
        "        \"    print(f\\\"Average reward over 100 episodes: {avg_reward:.3f}\\\")\\n\",\n",
        "        \"    print(f\\\"Average steps to termination: {avg_steps:.1f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    if args.env.startswith(\\\"FrozenLake\\\"):\\n\",\n",
        "        \"        print(\\\"\\\\nTips for FrozenLake:\\\")\\n\",\n",
        "        \"        print(\\\"- Slippery=True → Use >=20k episodes and slower epsilon decay (0.9997).\\\")\\n\",\n",
        "        \"        print(\\\"- Slippery=False → 3k–10k episodes often suffice.\\\")\\n\",\n",
        "        \"    elif args.env.startswith(\\\"Taxi\\\"):\\n\",\n",
        "        \"        print(\\\"\\\\nTips for Taxi:\\\")\\n\",\n",
        "        \"        print(\\\"- Learns faster: ~5k–10k episodes work well.\\\")\\n\",\n",
        "        \"        print(\\\"- Use slightly faster epsilon decay like 0.995.\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    np.save(\\\"q_table.npy\\\", q_table)\\n\",\n",
        "        \"    np.save(\\\"returns.npy\\\", returns)\\n\",\n",
        "        \"    np.save(\\\"epsilons.npy\\\", eps)\\n\",\n",
        "        \"    print(\\\"\\\\nSaved: q_table.npy, returns.npy, epsilons.npy\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"if __name__ == '__main__':\\n\",\n",
        "        \"    try:\\n\",\n",
        "        \"        main()\\n\",\n",
        "        \"    except KeyboardInterrupt:\\n\",\n",
        "        \"        print(\\\"Interrupted by user.\\\")\\n\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"g5W6o1qUCOTn\"\n",
        "      },\n",
        "      \"execution_count\": None,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dqVGuphIf-e",
        "outputId": "48766cf4-c1ce-469e-850d-8c3ab850a0f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nbformat': 4,\n",
              " 'nbformat_minor': 0,\n",
              " 'metadata': {'colab': {'provenance': [],\n",
              "   'authorship_tag': 'ABX9TyOymC+jmASj24GAG1/9MUXC',\n",
              "   'include_colab_link': True},\n",
              "  'kernelspec': {'name': 'python3', 'display_name': 'Python 3'},\n",
              "  'language_info': {'name': 'python'}},\n",
              " 'cells': [{'cell_type': 'markdown',\n",
              "   'metadata': {'id': 'view-in-github', 'colab_type': 'text'},\n",
              "   'source': ['<a href=\"https://colab.research.google.com/github/Goutham345/Reinforcement_Learning/blob/main/lab-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': 3,\n",
              "   'metadata': {'colab': {'base_uri': 'https://localhost:8080/'},\n",
              "    'id': 'i2KehKKYBT4R',\n",
              "    'outputId': 'b3003792-69de-46d0-f108-a1f8add4ea70'},\n",
              "   'outputs': [{'output_type': 'stream',\n",
              "     'name': 'stdout',\n",
              "     'text': ['Using backend: gymnasium\\n',\n",
              "      'Training FrozenLake-v1 | episodes=20000, alpha=0.8, gamma=0.95\\n',\n",
              "      '[  2000/20000] avg_return(last 200) = 0.035, epsilon=0.368\\n',\n",
              "      '[  4000/20000] avg_return(last 200) = 0.145, epsilon=0.135\\n',\n",
              "      '[  6000/20000] avg_return(last 200) = 0.205, epsilon=0.050\\n',\n",
              "      '[  8000/20000] avg_return(last 200) = 0.400, epsilon=0.018\\n',\n",
              "      '[ 10000/20000] avg_return(last 200) = 0.440, epsilon=0.010\\n',\n",
              "      '[ 12000/20000] avg_return(last 200) = 0.425, epsilon=0.010\\n',\n",
              "      '[ 14000/20000] avg_return(last 200) = 0.495, epsilon=0.010\\n',\n",
              "      '[ 16000/20000] avg_return(last 200) = 0.500, epsilon=0.010\\n',\n",
              "      '[ 18000/20000] avg_return(last 200) = 0.540, epsilon=0.010\\n',\n",
              "      '[ 20000/20000] avg_return(last 200) = 0.375, epsilon=0.010\\n',\n",
              "      '\\n',\n",
              "      'Evaluation (greedy policy):\\n',\n",
              "      'Average reward over 100 episodes: 0.240\\n',\n",
              "      'Average steps to termination: 16.1\\n',\n",
              "      '\\n',\n",
              "      'Tips for FrozenLake:\\n',\n",
              "      '- Slippery=True → Use >=20k episodes and slower epsilon decay (0.9997).\\n',\n",
              "      '- Slippery=False → 3k–10k episodes often suffice.\\n',\n",
              "      '\\n',\n",
              "      'Saved: q_table.npy, returns.npy, epsilons.npy\\n']}],\n",
              "   'source': ['import argparse\\n',\n",
              "    'import sys\\n',\n",
              "    'import math\\n',\n",
              "    'import numpy as np\\n',\n",
              "    'import random\\n',\n",
              "    '\\n',\n",
              "    '# Try Gymnasium first; fallback to Gym\\n',\n",
              "    'try:\\n',\n",
              "    '    import gymnasium as gym\\n',\n",
              "    '    GYMN = \"gymnasium\"\\n',\n",
              "    'except Exception:\\n',\n",
              "    '    import gym\\n',\n",
              "    '    GYMN = \"gym\"\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def make_env(env_id: str,\\n',\n",
              "    '             is_slippery: bool | None = None,\\n',\n",
              "    '             render_mode: str | None = None,\\n',\n",
              "    '             seed: int | None = 42):\\n',\n",
              "    '    \"\"\"Create an environment with sensible defaults for FrozenLake and Taxi.\"\"\"\\n',\n",
              "    '    kwargs = {}\\n',\n",
              "    '    if render_mode is not None:\\n',\n",
              "    '        kwargs[\"render_mode\"] = render_mode\\n',\n",
              "    '    if env_id.startswith(\"FrozenLake\") and is_slippery is not None:\\n',\n",
              "    '        kwargs[\"is_slippery\"] = bool(is_slippery)\\n',\n",
              "    '\\n',\n",
              "    '    env = gym.make(env_id, **kwargs)\\n',\n",
              "    '\\n',\n",
              "    '    try:\\n',\n",
              "    '        env.reset(seed=seed)\\n',\n",
              "    '    except TypeError:\\n',\n",
              "    '        pass\\n',\n",
              "    '    if hasattr(env, \"action_space\") and hasattr(env.action_space.seed):\\n',\n",
              "    '        env.action_space.seed(seed)\\n',\n",
              "    '    if hasattr(env, \"observation_space\") and hasattr(env.observation_space.n, \"seed\"):\\n',\n",
              "    '        env.observation_space.seed(seed)\\n',\n",
              "    '    return env\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def toint(x) -> int:\\n',\n",
              "    '    return int(x) if not isinstance(x, (tuple, list, np.ndarray)) else int(x[0])\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def greedyaction(q_row: np.ndarray) -> int:\\n',\n",
              "    '    max_val = np.max(q_row)\\n',\n",
              "    '    best_acts = np.flatnonzero(q_row == max_val)\\n',\n",
              "    '    return int(np.random.choice(best_acts))\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def epsilon_greedy_action(q_table: np.ndarray, state: int, epsilon: float, n_actions: int) -> int:\\n',\n",
              "    '    if random.random() < epsilon:\\n',\n",
              "    '        return random.randrange(n_actions)\\n',\n",
              "    '    return greedyaction(q_table[state])\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def stepunpack(step_out):\\n',\n",
              "    '    \"\"\"Unpack step results for Gymnasium (5-tuple) and Gym (4-tuple).\"\"\"\\n',\n",
              "    '    if isinstance(step_out, tuple):\\n',\n",
              "    '        if len(step_out) == 5:\\n',\n",
              "    '            next_state, reward, terminated, truncated, info = step_out\\n',\n",
              "    '            done = bool(terminated) or bool(truncated)\\n',\n",
              "    '            return next_state, reward, done\\n',\n",
              "    '        elif len(step_out) == 4:\\n',\n",
              "    '            next_state, reward, done, info = step_out\\n',\n",
              "    '            return next_state, reward, bool(done)\\n',\n",
              "    '    raise RuntimeError(\"Unexpected env.step(...) return format.\")\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def resetobs(reset_out):\\n',\n",
              "    '    return reset_out[0] if isinstance(reset_out, tuple) else reset_out\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def train_q_learning(env_id: str = \"FrozenLake-v1\",\\n',\n",
              "    '                     episodes: int = 20000,\\n',\n",
              "    '                     max_steps: int = 200,\\n',\n",
              "    '                     alpha: float = 0.8,\\n',\n",
              "    '                     gamma: float = 0.95,\\n',\n",
              "    '                     epsilon: float = 1.0,\\n',\n",
              "    '                     epsilon_min: float = 0.01,\\n',\n",
              "    '                     epsilon_decay: float = 0.9995,\\n',\n",
              "    '                     is_slippery: bool | None = None,\\n',\n",
              "    '                     seed: int = 42,\\n',\n",
              "    '                     verbose: bool = True):\\n',\n",
              "    '    env = make_env(env_id, is_slippery=is_slippery, seed=seed)\\n',\n",
              "    \"    assert hasattr(env.observation_space, 'n') and hasattr(env.action_space, 'n'), \\\\\\n\",\n",
              "    '        \"This Q-learning implementation expects discrete state and action spaces.\"\\n',\n",
              "    '\\n',\n",
              "    '    n_states = int(env.observation_space.n)\\n',\n",
              "    '    n_actions = int(env.action_space.n)\\n',\n",
              "    '\\n',\n",
              "    '    q_table = np.zeros((n_states, n_actions), dtype=np.float32)\\n',\n",
              "    '    returns = np.zeros(episodes, dtype=np.float32)\\n',\n",
              "    '    epsilons = np.zeros(episodes, dtype=np.float32)\\n',\n",
              "    '\\n',\n",
              "    '    for ep in range(episodes):\\n',\n",
              "    '        reset_out = env.reset(seed=seed + ep)\\n',\n",
              "    '        state = toint(resetobs(reset_out))\\n',\n",
              "    '\\n',\n",
              "    '        total_reward = 0.0\\n',\n",
              "    '        for t in range(max_steps):\\n',\n",
              "    '            action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\\n',\n",
              "    '            step_out = env.step(action)\\n',\n",
              "    '            next_state, reward, done = stepunpack(step_out)\\n',\n",
              "    '\\n',\n",
              "    '            s = toint(state)\\n',\n",
              "    '            s_next = toint(next_state)\\n',\n",
              "    '\\n',\n",
              "    '            # Q-learning update\\n',\n",
              "    '            best_next = float(np.max(q_table[s_next]))\\n',\n",
              "    '            td_target = float(reward) + (0.0 if done else gamma * best_next)\\n',\n",
              "    '            td_error = td_target - q_table[s, action]\\n',\n",
              "    '            q_table[s, action] += alpha * td_error\\n',\n",
              "    '\\n',\n",
              "    '            state = s_next\\n',\n",
              "    '            total_reward += float(reward)\\n',\n",
              "    '            if done:\\n',\n",
              "    '                break\\n',\n",
              "    '\\n',\n",
              "    '        returns[ep] = total_reward\\n',\n",
              "    '        epsilons[ep] = epsilon\\n',\n",
              "    '        epsilon = max(epsilon_min, epsilon * epsilon_decay)\\n',\n",
              "    '\\n',\n",
              "    '        if verbose and (ep + 1) % max(1, episodes // 10) == 0:\\n',\n",
              "    '            window = 200 if episodes >= 200 else max(1, episodes // 5)\\n',\n",
              "    '            avg_recent = float(np.mean(returns[max(0, ep - window + 1):ep + 1]))\\n',\n",
              "    '            print(f\"[{ep+1:6d}/{episodes}] avg_return(last {window}) = {avg_recent:.3f}, epsilon={epsilon:.3f}\")\\n',\n",
              "    '\\n',\n",
              "    '    env.close()\\n',\n",
              "    '    return q_table, returns, epsilons\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def evaluate_policy(env_id: str,\\n',\n",
              "    '                    q_table: np.ndarray,\\n',\n",
              "    '                    episodes: int = 100,\\n',\n",
              "    '                    max_steps: int = 200,\\n',\n",
              "    '                    is_slippery: bool | None = None,\\n',\n",
              "    '                    seed: int = 9999):\\n',\n",
              "    '    env = make_env(env_id, is_slippery=is_slippery, render_mode=None, seed=seed)\\n',\n",
              "    '\\n',\n",
              "    '    total_rewards = []\\n',\n",
              "    '    steps_taken = []\\n',\n",
              "    '\\n',\n",
              "    '    for ep in range(episodes):\\n',\n",
              "    '        reset_out = env.reset(seed=seed + ep)\\n',\n",
              "    '        state = toint(resetobs(reset_out))\\n',\n",
              "    '        ep_reward = 0.0\\n',\n",
              "    '        for t in range(max_steps):\\n',\n",
              "    '            action = greedyaction(q_table[state])\\n',\n",
              "    '            step_out = env.step(action)\\n',\n",
              "    '            next_state, reward, done = stepunpack(step_out)\\n',\n",
              "    '\\n',\n",
              "    '            ep_reward += float(reward)\\n',\n",
              "    '            state = toint(next_state)\\n',\n",
              "    '            if done:\\n',\n",
              "    '                steps_taken.append(t + 1)\\n',\n",
              "    '                break\\n',\n",
              "    '        else:\\n',\n",
              "    '            steps_taken.append(max_steps)\\n',\n",
              "    '        total_rewards.append(ep_reward)\\n',\n",
              "    '\\n',\n",
              "    '    env.close()\\n',\n",
              "    '    return float(np.mean(total_rewards)), float(np.mean(steps_taken))\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'def main():\\n',\n",
              "    '    parser = argparse.ArgumentParser(description=\"Tabular Q-Learning for discrete Gym environments\")\\n',\n",
              "    \"    parser.add_argument('--env', type=str, default='FrozenLake-v1')\\n\",\n",
              "    \"    parser.add_argument('--episodes', type=int, default=20000)\\n\",\n",
              "    \"    parser.add_argument('--max_steps', type=int, default=200)\\n\",\n",
              "    \"    parser.add_argument('--alpha', type=float, default=0.8)\\n\",\n",
              "    \"    parser.add_argument('--gamma', type=float, default=0.95)\\n\",\n",
              "    \"    parser.add_argument('--epsilon', type=float, default=1.0)\\n\",\n",
              "    \"    parser.add_argument('--epsilon_min', type=float, default=0.01)\\n\",\n",
              "    \"    parser.add_argument('--epsilon_decay', type=float, default=0.9995)\\n\",\n",
              "    \"    parser.add_argument('--is_slippery', type=int, default=None, choices=[0, 1])\\n\",\n",
              "    \"    parser.add_argument('--seed', type=int, default=42)\\n\",\n",
              "    \"    parser.add_argument('--no_verbose', action='store_true')\\n\",\n",
              "    '\\n',\n",
              "    '    # FIX for Jupyter/Colab extra arguments\\n',\n",
              "    '    args, _ = parser.parse_known_args()\\n',\n",
              "    '\\n',\n",
              "    '    print(f\"Using backend: {GYMN}\")\\n',\n",
              "    '    print(f\"Training {args.env} | episodes={args.episodes}, alpha={args.alpha}, gamma={args.gamma}\")\\n',\n",
              "    '\\n',\n",
              "    '    q_table, returns, eps = train_q_learning(env_id=args.env,\\n',\n",
              "    '                                             episodes=args.episodes,\\n',\n",
              "    '                                             max_steps=args.max_steps,\\n',\n",
              "    '                                             alpha=args.alpha,\\n',\n",
              "    '                                             gamma=args.gamma,\\n',\n",
              "    '                                             epsilon=args.epsilon,\\n',\n",
              "    '                                             epsilon_min=args.epsilon_min,\\n',\n",
              "    '                                             epsilon_decay=args.epsilon_decay,\\n',\n",
              "    '                                             is_slippery=(None if args.is_slippery is None else bool(args.is_slippery)),\\n',\n",
              "    '                                             seed=args.seed,\\n',\n",
              "    '                                             verbose=not args.no_verbose)\\n',\n",
              "    '\\n',\n",
              "    '    avg_reward, avg_steps = evaluate_policy(env_id=args.env,\\n',\n",
              "    '                                            q_table=q_table,\\n',\n",
              "    '                                            max_steps=args.max_steps,\\n',\n",
              "    '                                            is_slippery=(None if args.is_slippery is None else bool(args.is_slippery)))\\n',\n",
              "    '\\n',\n",
              "    '    print(\"\\\\nEvaluation (greedy policy):\")\\n',\n",
              "    '    print(f\"Average reward over 100 episodes: {avg_reward:.3f}\")\\n',\n",
              "    '    print(f\"Average steps to termination: {avg_steps:.1f}\")\\n',\n",
              "    '\\n',\n",
              "    '    if args.env.startswith(\"FrozenLake\"):\\n',\n",
              "    '        print(\"\\\\nTips for FrozenLake:\")\\n',\n",
              "    '        print(\"- Slippery=True → Use >=20k episodes and slower epsilon decay (0.9997).\")\\n',\n",
              "    '        print(\"- Slippery=False → 3k–10k episodes often suffice.\")\\n',\n",
              "    '    elif args.env.startswith(\"Taxi\"):\\n',\n",
              "    '        print(\"\\\\nTips for Taxi:\")\\n',\n",
              "    '        print(\"- Learns faster: ~5k–10k episodes work well.\")\\n',\n",
              "    '        print(\"- Use slightly faster epsilon decay like 0.995.\")\\n',\n",
              "    '\\n',\n",
              "    '    np.save(\"q_table.npy\", q_table)\\n',\n",
              "    '    np.save(\"returns.npy\", returns)\\n',\n",
              "    '    np.save(\"epsilons.npy\", eps)\\n',\n",
              "    '    print(\"\\\\nSaved: q_table.npy, returns.npy, epsilons.npy\")\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    \"if __name__ == '__main__':\\n\",\n",
              "    '    try:\\n',\n",
              "    '        main()\\n',\n",
              "    '    except KeyboardInterrupt:\\n',\n",
              "    '        print(\"Interrupted by user.\")\\n']},\n",
              "  {'cell_type': 'code',\n",
              "   'source': [],\n",
              "   'metadata': {'id': 'g5W6o1qUCOTn'},\n",
              "   'execution_count': None,\n",
              "   'outputs': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}