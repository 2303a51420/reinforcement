{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjqDn2cmej5V2/812dOG1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a51420/reinforcement/blob/main/lab%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v15rjxPnCl9v",
        "outputId": "e5ca3076-d2df-4859-f9fb-a5e7eeecc489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating baseline policy (stick on 20+, else hit)...\n",
            "Baseline policy average return (approx): -0.3648\n",
            "Learning with MC control (ε-greedy)... this may take a bit for many episodes.\n",
            "Final epsilon after training: 0.0500\n",
            "Greedy policy (from learned Q) average return (approx): -0.0555\n",
            "Q(20, 10, False) = [ 0.43704922 -0.87336245]\n",
            "Q(13, 2, False) = [-0.53846154 -0.39742765]\n",
            "Q(12, 6, True) = [-0.26666667 -1.        ]\n",
            "Q(18, 9, True) = [-0.21568627 -0.5       ]\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from typing import Dict, Tuple, List, Callable\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "\n",
        "def make_epsilon_greedy_policy(Q: Dict, nA: int, epsilon: float) -> Callable:\n",
        "\n",
        "    \"\"\"Return a policy function that takes state and returns action probabilities.\"\"\"\n",
        "\n",
        "    def policy_fn(state):\n",
        "\n",
        "        # Ensure state exists in Q\n",
        "\n",
        "        _ = Q[state]  # triggers defaultdict to create if missing\n",
        "\n",
        "        probs = np.ones(nA, dtype=float) * (epsilon / nA)\n",
        "\n",
        "        best_a = int(np.argmax(Q[state]))\n",
        "\n",
        "        probs[best_a] += (1.0 - epsilon)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    return policy_fn\n",
        "\n",
        "def generate_episode(env, policy_fn: Callable) -> List[Tuple[Tuple, int, float]]:\n",
        "\n",
        "    \"\"\"Generate an episode: returns list of (state, action, reward). Uses policy as action-prob function.\"\"\"\n",
        "\n",
        "    episode: List[Tuple[Tuple, int, float]] = []\n",
        "\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        probs = policy_fn(state)\n",
        "\n",
        "        action = int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "        episode.append((state, action, reward))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    return episode\n",
        "\n",
        "# ---------- First-Visit MC Policy Evaluation ----------\n",
        "\n",
        "def first_visit_mc_policy_evaluation(env, policy_fn: Callable, gamma: float = 1.0, num_episodes: int = 10000):\n",
        "    \"\"\"\n",
        "    First-Visit MC Policy Evaluation. Estimates V for a given policy.\n",
        "\n",
        "    Args:\n",
        "        env: The Gymnasium environment.\n",
        "        policy_fn: A function that takes a state and returns action probabilities.\n",
        "        gamma: Discount factor.\n",
        "        num_episodes: Number of episodes to generate.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping states to estimated values.\n",
        "    \"\"\"\n",
        "\n",
        "    # State-Value function\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    # Sum of returns for each state, and count of visits for each state\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Generate an episode\n",
        "        episode = generate_episode(env, policy_fn)\n",
        "\n",
        "        # Track first-visit indices for (state)\n",
        "        state_first_visit = {}\n",
        "        for idx, (s, _, _) in enumerate(episode):\n",
        "            if s not in state_first_visit:\n",
        "                state_first_visit[s] = idx\n",
        "\n",
        "        # For each first-visited state, compute the return and update the value\n",
        "        for state, first_idx in state_first_visit.items():\n",
        "            # Calculate the return\n",
        "            G = 0.0\n",
        "            power = 0\n",
        "            for (_, _, r) in episode[first_idx:]:\n",
        "                G += (gamma ** power) * r\n",
        "                power += 1\n",
        "\n",
        "            returns_sum[state] += G\n",
        "            returns_count[state] += 1.0\n",
        "            V[state] = returns_sum[state] / returns_count[state]\n",
        "\n",
        "    return V\n",
        "\n",
        "# ---------- On-Policy MC Control with ε-greedy ----------\n",
        "\n",
        "def mc_control_epsilon_greedy(\n",
        "\n",
        "    env,\n",
        "\n",
        "    num_episodes: int = 500000,\n",
        "\n",
        "    gamma: float = 1.0,\n",
        "\n",
        "    epsilon_start: float = 1.0,\n",
        "\n",
        "    epsilon_min: float = 0.05,\n",
        "\n",
        "    epsilon_decay: float = 0.9995,\n",
        "\n",
        "):\n",
        "\n",
        "\n",
        "    nA = env.action_space.n\n",
        "\n",
        "    Q = defaultdict(lambda: np.zeros(nA, dtype=float))\n",
        "\n",
        "    returns_sum = defaultdict(float)          # keyed by (state, action)\n",
        "\n",
        "    returns_count = defaultdict(float)        # keyed by (state, action)\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    def policy_for_episode(state):\n",
        "\n",
        "        # policy used while generating episodes (current ε)\n",
        "\n",
        "        probs = np.ones(nA, dtype=float) * (epsilon / nA)\n",
        "\n",
        "        best_a = int(np.argmax(Q[state]))\n",
        "\n",
        "        probs[best_a] += (1.0 - epsilon)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "\n",
        "        episode = generate_episode(env, policy_for_episode)\n",
        "\n",
        "        # Track first-visit indices for (s,a)\n",
        "\n",
        "        sa_first_visit = {}\n",
        "\n",
        "        for idx, (s, a, _) in enumerate(episode):\n",
        "\n",
        "            if (s, a) not in sa_first_visit:\n",
        "\n",
        "                sa_first_visit[(s, a)] = idx\n",
        "\n",
        "        # For each first-visited (s,a), compute return and update\n",
        "\n",
        "        for (s, a), first_idx in sa_first_visit.items():\n",
        "\n",
        "            G = 0.0\n",
        "\n",
        "            power = 0\n",
        "\n",
        "            for (_, _, r) in episode[first_idx:]:\n",
        "\n",
        "                G += (gamma ** power) * r\n",
        "\n",
        "                power += 1\n",
        "\n",
        "            returns_sum[(s, a)] += G\n",
        "\n",
        "            returns_count[(s, a)] += 1.0\n",
        "\n",
        "            Q[s][a] = returns_sum[(s, a)] / returns_count[(s, a)]\n",
        "\n",
        "        # Decay epsilon\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Return a policy function tied to the learned Q with the final epsilon\n",
        "\n",
        "    learned_policy = make_epsilon_greedy_policy(Q, env.action_space.n, epsilon)\n",
        "\n",
        "    return Q, learned_policy, epsilon\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "\n",
        "def greedy_policy_from_Q(Q: Dict, nA: int) -> Callable:\n",
        "\n",
        "    \"\"\"Deterministic greedy policy from Q.\"\"\"\n",
        "\n",
        "    def policy_fn(state):\n",
        "\n",
        "        _ = Q[state]  # ensure state exists\n",
        "\n",
        "        probs = np.zeros(nA, dtype=float)\n",
        "\n",
        "        best_a = int(np.argmax(Q[state]))\n",
        "\n",
        "        probs[best_a] = 1.0\n",
        "\n",
        "        return probs\n",
        "\n",
        "    return policy_fn\n",
        "\n",
        "def evaluate_policy_avg_return(env, policy_fn: Callable, episodes: int = 10000) -> float:\n",
        "\n",
        "    \"\"\"Estimate average return by running the given policy for a number of episodes.\"\"\"\n",
        "\n",
        "    total = 0.0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "\n",
        "        ep = generate_episode(env, policy_fn)\n",
        "\n",
        "        total += sum(r for (_, _, r) in ep)\n",
        "\n",
        "    return total / episodes\n",
        "\n",
        "# ---------- Main ----------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create Blackjack env (Gymnasium)\n",
        "\n",
        "    # Action space: 0 = Stick, 1 = Hit\n",
        "\n",
        "    env = gym.make(\"Blackjack-v1\")  # uses toy_text Blackjack\n",
        "\n",
        "    nA = env.action_space.n\n",
        "\n",
        "    # (A) Example: Evaluate a simple baseline policy (stick on 20+ else hit)\n",
        "\n",
        "    def baseline_policy(state):\n",
        "\n",
        "        player_sum, dealer_card, usable_ace = state\n",
        "\n",
        "        action = 0 if player_sum >= 20 else 1\n",
        "\n",
        "        probs = np.zeros(nA, dtype=float)\n",
        "\n",
        "        probs[action] = 1.0\n",
        "\n",
        "        return probs\n",
        "\n",
        "    print(\"Evaluating baseline policy (stick on 20+, else hit)...\")\n",
        "\n",
        "    V_baseline = first_visit_mc_policy_evaluation(env, baseline_policy, gamma=1.0, num_episodes=20000)\n",
        "\n",
        "    avg_return_baseline = evaluate_policy_avg_return(env, baseline_policy, episodes=10000)\n",
        "\n",
        "    print(f\"Baseline policy average return (approx): {avg_return_baseline:.4f}\")\n",
        "\n",
        "    # (B) Learn via MC control ε-greedy\n",
        "\n",
        "    print(\"Learning with MC control (ε-greedy)... this may take a bit for many episodes.\")\n",
        "\n",
        "    Q, learned_eps_policy, final_eps = mc_control_epsilon_greedy(\n",
        "\n",
        "        env,\n",
        "\n",
        "        num_episodes=200000,   # increase for stronger results\n",
        "\n",
        "        gamma=1.0,\n",
        "\n",
        "        epsilon_start=1.0,\n",
        "\n",
        "        epsilon_min=0.05,\n",
        "\n",
        "        epsilon_decay=0.9995,\n",
        "\n",
        "    )\n",
        "\n",
        "    greedy_pi = greedy_policy_from_Q(Q, nA)\n",
        "\n",
        "    avg_return_greedy = evaluate_policy_avg_return(env, greedy_pi, episodes=20000)\n",
        "\n",
        "    print(f\"Final epsilon after training: {final_eps:.4f}\")\n",
        "\n",
        "    print(f\"Greedy policy (from learned Q) average return (approx): {avg_return_greedy:.4f}\")\n",
        "\n",
        "    # Example: show a few Q-values\n",
        "\n",
        "    sample_states = [\n",
        "\n",
        "        (20, 10, False),  # player 20 vs dealer 10, no usable ace\n",
        "\n",
        "        (13, 2, False),\n",
        "\n",
        "        (12, 6, True),\n",
        "\n",
        "        (18, 9, True),\n",
        "\n",
        "    ]\n",
        "\n",
        "    for s in sample_states:\n",
        "\n",
        "        _ = Q[s]  # ensure exists\n",
        "\n",
        "        print(f\"Q{str(s)} = {Q[s]}\")"
      ]
    }
  ]
}